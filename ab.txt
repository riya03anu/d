//1 ------------------
#plt.figure(figsize=(16,9))
plt.title('Temperature Plot of INDIA')
plt.xlabel('Year')
plt.ylabel('Anuual Average Temperature')
plt.scatter(x, y, label = 'actual', color='r')
plt.plot(x, predicted, label = 'predicted', color='g')
plt.legend()


//2-----------------------

from sklearn.preprocessing import Binarizer bi Binarizer (threshold=0.75) df['Chance of Admit'] = bi.fit_transform(df [['Chance of Admit '11)

df.head()

x = df.drop('Chance of Admit, axis= 1)
y=df['Chance of Admit']

X

y = y.astype('int')
sns.countplot(xy);

# Cross-validation

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, random_state=0, test_size=0.25)

x_train.shape
x_test.shape
x_test

# Import the class from sklearn.tree import DecisionTreeClassifier

classifier = DecisionTreeClassifier (random_state=0)

 classifier.fit(x_train, y_train)

 DecisionTreeClassifier (random_state=0)

y_pred = classifier.predict(x_test)

result = pd.DataFrame({
'actual' y_test, 'predicted' y_pred })

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt plt.figure(figsize=(12,12)) plot_tree (classifier, fontsize=8, filled=True, rounded=True, feature_names=x.columns, class_names=['NA', 'AD']);

4----------------------
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier

# Load the SMS spam dataset
df = pd.read_csv('/content/spam.csv', encoding='ISO-8859-1')

# Create a binary target variable 'spam'
df['spam'] = df['v1'].apply(lambda x: 1 if x == 'spam' else 0)

# Select relevant columns
new_df = df[['v1', 'v2', 'spam']

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(new_df['v2'], new_df['spam'], test_size=0.2, random_state=42)

# Convert text messages to numerical features using CountVectorizer
vectorizer = CountVectorizer()
x_train_count = vectorizer.fit_transform(x_train)
x_test_count = vectorizer.transform(x_test)

# Train a Multinomial Naive Bayes classifier
naive_bayes_model = MultinomialNB()
naive_bayes_model.fit(x_train_count, y_train)

# Train a Random Forest classifier
random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)
random_forest_model.fit(x_train_count, y_train)

# Evaluate the Multinomial Naive Bayes model
naive_bayes_accuracy = naive_bayes_model.score(x_test_count, y_test)
print("Multinomial Naive Bayes Accuracy:", naive_bayes_accuracy)

# Evaluate the Random Forest model
random_forest_accuracy = random_forest_model.score(x_test_count, y_test)
print("Random Forest Accuracy:", random_forest_accuracy)


5--------------------------------

x = df.iloc[:,3:]
x
plt.title('Unclustered Data')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.scatter (x['Annual Income (ks)'], x['Spending Score (1-100)'])

from sklearn.cluster import KMeans, AgglomerativeClustering

km = KMeans (n_clusters=3)

x.shape

km.fit_predict(x)

#SSE 
km.inertia_

sse = []
for k in range(1,16):
km = KMeans (n_clusters=k) km.fit_predict(x) sse.append(km.inertia_)

Sse

plt.title('Elbow Method')
plt.xlabel('Value of K')
plt.ylabel('SSE')
plt.grid()
plt.xticks(range(1,16))
plt.plot(range(1,16), sse, marker='.', color='red')

from sklearn.metrics import silhouette_score

silh = []
for k in range(2,16):
km = KMeans (n_clusters=k)
labels = km.fit_predict(x)
score silhouette_score (x, labels) silh.append(score)

plt.title('Silhoutte Method')
plt.xlabel('Value of K')
plt.ylabel('Silhoutte Score')
plt.grid()
plt.xticks(range(2,16)) 1
plt.bar(range(2,16), silh, color='red')

km = KMeans (n_clusters=5, random_state=0)

labels = km. fit_predict(x)
labels

cent = km.cluster_centers_

 plt.figure(figsize=(16,9))
plt.subplot(1,2,1)
plt.title('Unclustered Data')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.scatter (x['Annual Income (k$)'], x['Spending Score (1-100)'])

plt.subplot(1,2,2)
plt.title('Clustered Data')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.scatter (x['Annual Income (k$)'], x['Spending Score (1-100)'], c=labels) 
plt.scatter (cent[:,0], cent[:,1], s=50, color='k')


//6------------------------

import pandas as pd import csv from mlxtend.preprocessing import TransactionEncoder from alxtend. frequent patterns import apriori, association rules

 dataset [] 
with open('Market Basket Optimisation.csv') as file: reader = csv.reader (file, delimiter=',') for row in reader:
dataset += [row]

dataset[]

len(dataset)

te= TransactionEncoder()
 x = te.fit_transform(dataset)

x

df = pd.DataFrame(x, columns=te.columns_)

Len(te.columns_)

df.head()

# 1. Find frequent itemsets

freq_itemset = apriori (df, min_support=0.01, use colnames=True)

# Find the rules rules association_rules (freq_itemset, metric-'confidence', min_threshold=0.25)